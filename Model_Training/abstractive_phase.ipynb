{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T02:31:30.167208Z","iopub.execute_input":"2025-03-20T02:31:30.167668Z","iopub.status.idle":"2025-03-20T02:31:30.532338Z","shell.execute_reply.started":"2025-03-20T02:31:30.167618Z","shell.execute_reply":"2025-03-20T02:31:30.531376Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T02:40:38.405332Z","iopub.execute_input":"2025-03-20T02:40:38.405831Z","iopub.status.idle":"2025-03-20T02:40:43.490870Z","shell.execute_reply.started":"2025-03-20T02:40:38.405799Z","shell.execute_reply":"2025-03-20T02:40:43.489773Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.17.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!gdown --id 11EQcClmtteji5TIrD-oHowxcD9K8NGyx\n!gdown --id 1PtC-WATPtlpkgO2uY8PyZbZCGPBaPcQD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T02:47:11.516475Z","iopub.execute_input":"2025-03-20T02:47:11.516818Z","iopub.status.idle":"2025-03-20T02:47:22.608700Z","shell.execute_reply.started":"2025-03-20T02:47:11.516792Z","shell.execute_reply":"2025-03-20T02:47:22.607826Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1PtC-WATPtlpkgO2uY8PyZbZCGPBaPcQD\nFrom (redirected): https://drive.google.com/uc?id=1PtC-WATPtlpkgO2uY8PyZbZCGPBaPcQD&confirm=t&uuid=4fb7c491-4fec-463d-914f-70a9ecdbecc0\nTo: /kaggle/working/train_part2_lexrank.csv\n100%|███████████████████████████████████████| 1.23G/1.23G [00:05<00:00, 222MB/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSVs\npart1 = pd.read_csv(\"train_part1_lexrank.csv\")\npart2 = pd.read_csv(\"train_part2_lexrank.csv\")\n\n# Combine them into a single DataFrame\ntrain_df = pd.concat([part1, part2]).reset_index(drop=True)\n\nprint(f\"Combined DataFrame shape: {train_df.shape}\")\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T02:50:11.843527Z","iopub.execute_input":"2025-03-20T02:50:11.843873Z","iopub.status.idle":"2025-03-20T02:50:35.984061Z","shell.execute_reply.started":"2025-03-20T02:50:11.843844Z","shell.execute_reply":"2025-03-20T02:50:35.983266Z"}},"outputs":[{"name":"stdout","text":"Combined DataFrame shape: (117108, 3)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                             article  \\\n0  a recent systematic analysis showed that in 20...   \n1  it occurs in more than 50% of patients and may...   \n2  tardive dystonia ( td ) , a rarer side effect ...   \n3  lepidoptera include agricultural pests that , ...   \n4  syncope is caused by transient diffuse cerebra...   \n\n                                            abstract  \\\n0  background : the present study was carried out...   \n1  backgroundanemia in patients with cancer who a...   \n2  tardive dystonia ( td ) is a serious side effe...   \n3  many lepidopteran insects are agricultural pes...   \n4  we present an unusual case of recurrent cough ...   \n\n                                  extractive_summary  \n0  therefore , the present study determines the e...  \n1  patients received the study treatment for 12 w...  \n2  as neck dystonia increased , the patient had a...  \n3  although immune pathways can be generally and ...  \n4  cough syncope , a rare form of syncope , may b...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>abstract</th>\n      <th>extractive_summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a recent systematic analysis showed that in 20...</td>\n      <td>background : the present study was carried out...</td>\n      <td>therefore , the present study determines the e...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>it occurs in more than 50% of patients and may...</td>\n      <td>backgroundanemia in patients with cancer who a...</td>\n      <td>patients received the study treatment for 12 w...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tardive dystonia ( td ) , a rarer side effect ...</td>\n      <td>tardive dystonia ( td ) is a serious side effe...</td>\n      <td>as neck dystonia increased , the patient had a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>lepidoptera include agricultural pests that , ...</td>\n      <td>many lepidopteran insects are agricultural pes...</td>\n      <td>although immune pathways can be generally and ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>syncope is caused by transient diffuse cerebra...</td>\n      <td>we present an unusual case of recurrent cough ...</td>\n      <td>cough syncope , a rare form of syncope , may b...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Initialize the tokenizer\nmodel_checkpoint = \"t5-small\"  # or t5-base / t5-large if needed\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# Ensure padding is handled correctly\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T02:50:47.880458Z","iopub.execute_input":"2025-03-20T02:50:47.880812Z","iopub.status.idle":"2025-03-20T02:50:59.010947Z","shell.execute_reply.started":"2025-03-20T02:50:47.880784Z","shell.execute_reply":"2025-03-20T02:50:59.010194Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e7a147f74654c579f340395d4328894"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d0d4c14f1b49578358168c7e65c948"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb63f8760044500a6bbd60b990fe050"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from datasets import Dataset\n\n# Convert DataFrame to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\n\n# (Optional) Inspect column names if needed\n# print(train_dataset.column_names)\n\ndef preprocess_function(examples):\n    # Add the T5 \"summarize: \" prefix to the input text\n    inputs = [\"summarize: \" + doc for doc in examples[\"extractive_summary\"]]\n    targets = examples[\"abstract\"]\n    \n    # Tokenize the inputs (truncated to T5's max length)\n    model_inputs = tokenizer(\n        inputs, \n        max_length=512, \n        padding=\"max_length\", \n        truncation=True\n    )\n    \n    # Tokenize the labels/targets (use reasonable max length)\n    labels = tokenizer(\n        targets, \n        max_length=209,  # Abstracts are often shorter than inputs\n        padding=\"max_length\", \n        truncation=True\n    )\n    \n    # Replace padding token id's in the labels with -100\n    labels[\"input_ids\"] = [\n        [(label if label != tokenizer.pad_token_id else -100) for label in label_ids]\n        for label_ids in labels[\"input_ids\"]\n    ]\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T02:55:20.297703Z","iopub.execute_input":"2025-03-20T02:55:20.298053Z","iopub.status.idle":"2025-03-20T02:55:42.674946Z","shell.execute_reply.started":"2025-03-20T02:55:20.298027Z","shell.execute_reply":"2025-03-20T02:55:42.673861Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\ntokenized_train_dataset.save_to_disk(\"t5_tokenized_train_dataset\")\n# Check one sample\ntokenized_train_dataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    T5Tokenizer, \n    T5ForConditionalGeneration, \n    TrainingArguments, \n    Trainer, \n    DataCollatorForSeq2Seq\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T03:06:38.932812Z","iopub.execute_input":"2025-03-20T03:06:38.933213Z","iopub.status.idle":"2025-03-20T03:06:56.164832Z","shell.execute_reply.started":"2025-03-20T03:06:38.933165Z","shell.execute_reply":"2025-03-20T03:06:56.163632Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Load T5 model\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n# Check if GPU is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nprint(\"Using device:\", device)\n\n# Data collator for batch processing\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T03:07:09.800361Z","iopub.execute_input":"2025-03-20T03:07:09.801028Z","iopub.status.idle":"2025-03-20T03:07:12.570378Z","shell.execute_reply.started":"2025-03-20T03:07:09.801000Z","shell.execute_reply":"2025-03-20T03:07:12.569550Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6690274c81f34dedb7e884657db36b33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23963bb81fb5462ca37937ef3f0859be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0bd34bae3af4d4a9327427222b059d7"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=3e-5,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    optim=\"adafactor\",\n    fp16=True,\n    push_to_hub=False,\n    dataloader_num_workers=4,\n    report_to=\"none\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T03:09:43.139602Z","iopub.execute_input":"2025-03-20T03:09:43.139984Z","iopub.status.idle":"2025-03-20T03:09:43.176771Z","shell.execute_reply.started":"2025-03-20T03:09:43.139951Z","shell.execute_reply":"2025-03-20T03:09:43.175763Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    data_collator=data_collator\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T03:18:10.659091Z","iopub.execute_input":"2025-03-20T03:18:10.659475Z","iopub.status.idle":"2025-03-20T10:11:02.146208Z","shell.execute_reply.started":"2025-03-20T03:18:10.659451Z","shell.execute_reply":"2025-03-20T10:11:02.145262Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='73195' max='73195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [73195/73195 6:52:47, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>3.452800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.212500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.151000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.111000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>3.093200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>3.045300</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>3.029000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>3.007300</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>2.994100</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.985400</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>2.985800</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>2.967300</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>2.951000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>2.941800</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>2.941600</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>2.939200</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>2.906400</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>2.910000</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>2.896000</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>2.907600</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>2.917100</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>2.885400</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>2.890300</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>2.880300</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>2.884600</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>2.874200</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>2.868100</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>2.866200</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>2.851800</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>2.843300</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>2.839000</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>2.841700</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>2.839800</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>2.834000</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>2.833400</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>2.838600</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>2.824400</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>2.827300</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>2.822900</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>2.822200</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>2.826800</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>2.824000</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>2.812500</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>2.807700</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>2.795600</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>2.810600</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>2.793700</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>2.810800</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>2.796900</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>2.805400</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>2.809200</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>2.795200</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>2.793200</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>2.794600</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>2.780100</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>2.787800</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>2.792100</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>2.787600</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>2.788000</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>2.780800</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>2.776900</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>2.773400</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>2.765800</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>2.769800</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>2.769800</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>2.769800</td>\n    </tr>\n    <tr>\n      <td>33500</td>\n      <td>2.767900</td>\n    </tr>\n    <tr>\n      <td>34000</td>\n      <td>2.747600</td>\n    </tr>\n    <tr>\n      <td>34500</td>\n      <td>2.764000</td>\n    </tr>\n    <tr>\n      <td>35000</td>\n      <td>2.755100</td>\n    </tr>\n    <tr>\n      <td>35500</td>\n      <td>2.773200</td>\n    </tr>\n    <tr>\n      <td>36000</td>\n      <td>2.764400</td>\n    </tr>\n    <tr>\n      <td>36500</td>\n      <td>2.760300</td>\n    </tr>\n    <tr>\n      <td>37000</td>\n      <td>2.758900</td>\n    </tr>\n    <tr>\n      <td>37500</td>\n      <td>2.750600</td>\n    </tr>\n    <tr>\n      <td>38000</td>\n      <td>2.748600</td>\n    </tr>\n    <tr>\n      <td>38500</td>\n      <td>2.764100</td>\n    </tr>\n    <tr>\n      <td>39000</td>\n      <td>2.750800</td>\n    </tr>\n    <tr>\n      <td>39500</td>\n      <td>2.743200</td>\n    </tr>\n    <tr>\n      <td>40000</td>\n      <td>2.744700</td>\n    </tr>\n    <tr>\n      <td>40500</td>\n      <td>2.748200</td>\n    </tr>\n    <tr>\n      <td>41000</td>\n      <td>2.743500</td>\n    </tr>\n    <tr>\n      <td>41500</td>\n      <td>2.752300</td>\n    </tr>\n    <tr>\n      <td>42000</td>\n      <td>2.750000</td>\n    </tr>\n    <tr>\n      <td>42500</td>\n      <td>2.747900</td>\n    </tr>\n    <tr>\n      <td>43000</td>\n      <td>2.729100</td>\n    </tr>\n    <tr>\n      <td>43500</td>\n      <td>2.741400</td>\n    </tr>\n    <tr>\n      <td>44000</td>\n      <td>2.740100</td>\n    </tr>\n    <tr>\n      <td>44500</td>\n      <td>2.732600</td>\n    </tr>\n    <tr>\n      <td>45000</td>\n      <td>2.745500</td>\n    </tr>\n    <tr>\n      <td>45500</td>\n      <td>2.723700</td>\n    </tr>\n    <tr>\n      <td>46000</td>\n      <td>2.738400</td>\n    </tr>\n    <tr>\n      <td>46500</td>\n      <td>2.744200</td>\n    </tr>\n    <tr>\n      <td>47000</td>\n      <td>2.735700</td>\n    </tr>\n    <tr>\n      <td>47500</td>\n      <td>2.725500</td>\n    </tr>\n    <tr>\n      <td>48000</td>\n      <td>2.737800</td>\n    </tr>\n    <tr>\n      <td>48500</td>\n      <td>2.727900</td>\n    </tr>\n    <tr>\n      <td>49000</td>\n      <td>2.734300</td>\n    </tr>\n    <tr>\n      <td>49500</td>\n      <td>2.726800</td>\n    </tr>\n    <tr>\n      <td>50000</td>\n      <td>2.741300</td>\n    </tr>\n    <tr>\n      <td>50500</td>\n      <td>2.723700</td>\n    </tr>\n    <tr>\n      <td>51000</td>\n      <td>2.728500</td>\n    </tr>\n    <tr>\n      <td>51500</td>\n      <td>2.728900</td>\n    </tr>\n    <tr>\n      <td>52000</td>\n      <td>2.726100</td>\n    </tr>\n    <tr>\n      <td>52500</td>\n      <td>2.730300</td>\n    </tr>\n    <tr>\n      <td>53000</td>\n      <td>2.719000</td>\n    </tr>\n    <tr>\n      <td>53500</td>\n      <td>2.720800</td>\n    </tr>\n    <tr>\n      <td>54000</td>\n      <td>2.731500</td>\n    </tr>\n    <tr>\n      <td>54500</td>\n      <td>2.720700</td>\n    </tr>\n    <tr>\n      <td>55000</td>\n      <td>2.722200</td>\n    </tr>\n    <tr>\n      <td>55500</td>\n      <td>2.729900</td>\n    </tr>\n    <tr>\n      <td>56000</td>\n      <td>2.730100</td>\n    </tr>\n    <tr>\n      <td>56500</td>\n      <td>2.710600</td>\n    </tr>\n    <tr>\n      <td>57000</td>\n      <td>2.734800</td>\n    </tr>\n    <tr>\n      <td>57500</td>\n      <td>2.715500</td>\n    </tr>\n    <tr>\n      <td>58000</td>\n      <td>2.716300</td>\n    </tr>\n    <tr>\n      <td>58500</td>\n      <td>2.705300</td>\n    </tr>\n    <tr>\n      <td>59000</td>\n      <td>2.717900</td>\n    </tr>\n    <tr>\n      <td>59500</td>\n      <td>2.712600</td>\n    </tr>\n    <tr>\n      <td>60000</td>\n      <td>2.728000</td>\n    </tr>\n    <tr>\n      <td>60500</td>\n      <td>2.694700</td>\n    </tr>\n    <tr>\n      <td>61000</td>\n      <td>2.706300</td>\n    </tr>\n    <tr>\n      <td>61500</td>\n      <td>2.728900</td>\n    </tr>\n    <tr>\n      <td>62000</td>\n      <td>2.721900</td>\n    </tr>\n    <tr>\n      <td>62500</td>\n      <td>2.731000</td>\n    </tr>\n    <tr>\n      <td>63000</td>\n      <td>2.711200</td>\n    </tr>\n    <tr>\n      <td>63500</td>\n      <td>2.704500</td>\n    </tr>\n    <tr>\n      <td>64000</td>\n      <td>2.716000</td>\n    </tr>\n    <tr>\n      <td>64500</td>\n      <td>2.717800</td>\n    </tr>\n    <tr>\n      <td>65000</td>\n      <td>2.712200</td>\n    </tr>\n    <tr>\n      <td>65500</td>\n      <td>2.711100</td>\n    </tr>\n    <tr>\n      <td>66000</td>\n      <td>2.707700</td>\n    </tr>\n    <tr>\n      <td>66500</td>\n      <td>2.721000</td>\n    </tr>\n    <tr>\n      <td>67000</td>\n      <td>2.701400</td>\n    </tr>\n    <tr>\n      <td>67500</td>\n      <td>2.727100</td>\n    </tr>\n    <tr>\n      <td>68000</td>\n      <td>2.722100</td>\n    </tr>\n    <tr>\n      <td>68500</td>\n      <td>2.710300</td>\n    </tr>\n    <tr>\n      <td>69000</td>\n      <td>2.721600</td>\n    </tr>\n    <tr>\n      <td>69500</td>\n      <td>2.717800</td>\n    </tr>\n    <tr>\n      <td>70000</td>\n      <td>2.725500</td>\n    </tr>\n    <tr>\n      <td>70500</td>\n      <td>2.713400</td>\n    </tr>\n    <tr>\n      <td>71000</td>\n      <td>2.699300</td>\n    </tr>\n    <tr>\n      <td>71500</td>\n      <td>2.704800</td>\n    </tr>\n    <tr>\n      <td>72000</td>\n      <td>2.707100</td>\n    </tr>\n    <tr>\n      <td>72500</td>\n      <td>2.692900</td>\n    </tr>\n    <tr>\n      <td>73000</td>\n      <td>2.711400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=73195, training_loss=2.797616304264294, metrics={'train_runtime': 24770.8139, 'train_samples_per_second': 23.638, 'train_steps_per_second': 2.955, 'total_flos': 7.924803843391488e+16, 'train_loss': 2.797616304264294, 'epoch': 5.0})"},"metadata":{}}],"execution_count":23}]}