{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 43917,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03415533847940433,
      "grad_norm": 93698.84375,
      "learning_rate": 2.9795067969123577e-05,
      "loss": 3.4528,
      "step": 500
    },
    {
      "epoch": 0.06831067695880866,
      "grad_norm": 517639.40625,
      "learning_rate": 2.959013593824715e-05,
      "loss": 3.2125,
      "step": 1000
    },
    {
      "epoch": 0.10246601543821299,
      "grad_norm": 60829.51171875,
      "learning_rate": 2.9385203907370722e-05,
      "loss": 3.151,
      "step": 1500
    },
    {
      "epoch": 0.13662135391761732,
      "grad_norm": 71213.7109375,
      "learning_rate": 2.9180271876494295e-05,
      "loss": 3.111,
      "step": 2000
    },
    {
      "epoch": 0.17077669239702165,
      "grad_norm": 70620.40625,
      "learning_rate": 2.897533984561787e-05,
      "loss": 3.0932,
      "step": 2500
    },
    {
      "epoch": 0.20493203087642597,
      "grad_norm": 81805.4453125,
      "learning_rate": 2.8770407814741447e-05,
      "loss": 3.0453,
      "step": 3000
    },
    {
      "epoch": 0.2390873693558303,
      "grad_norm": 74381.703125,
      "learning_rate": 2.856547578386502e-05,
      "loss": 3.029,
      "step": 3500
    },
    {
      "epoch": 0.27324270783523463,
      "grad_norm": 60440.85546875,
      "learning_rate": 2.8360543752988595e-05,
      "loss": 3.0073,
      "step": 4000
    },
    {
      "epoch": 0.30739804631463896,
      "grad_norm": 70851.53125,
      "learning_rate": 2.8155611722112164e-05,
      "loss": 2.9941,
      "step": 4500
    },
    {
      "epoch": 0.3415533847940433,
      "grad_norm": 67819.2265625,
      "learning_rate": 2.795067969123574e-05,
      "loss": 2.9854,
      "step": 5000
    },
    {
      "epoch": 0.3757087232734476,
      "grad_norm": 64966.30078125,
      "learning_rate": 2.7745747660359316e-05,
      "loss": 2.9858,
      "step": 5500
    },
    {
      "epoch": 0.40986406175285195,
      "grad_norm": 59541.2265625,
      "learning_rate": 2.754081562948289e-05,
      "loss": 2.9673,
      "step": 6000
    },
    {
      "epoch": 0.4440194002322563,
      "grad_norm": 71041.578125,
      "learning_rate": 2.7335883598606465e-05,
      "loss": 2.951,
      "step": 6500
    },
    {
      "epoch": 0.4781747387116606,
      "grad_norm": 71348.7890625,
      "learning_rate": 2.7130951567730034e-05,
      "loss": 2.9418,
      "step": 7000
    },
    {
      "epoch": 0.5123300771910649,
      "grad_norm": 66646.8203125,
      "learning_rate": 2.692601953685361e-05,
      "loss": 2.9416,
      "step": 7500
    },
    {
      "epoch": 0.5464854156704693,
      "grad_norm": 99987.984375,
      "learning_rate": 2.6721087505977186e-05,
      "loss": 2.9392,
      "step": 8000
    },
    {
      "epoch": 0.5806407541498736,
      "grad_norm": 70325.953125,
      "learning_rate": 2.651615547510076e-05,
      "loss": 2.9064,
      "step": 8500
    },
    {
      "epoch": 0.6147960926292779,
      "grad_norm": 74620.1953125,
      "learning_rate": 2.6311223444224335e-05,
      "loss": 2.91,
      "step": 9000
    },
    {
      "epoch": 0.6489514311086823,
      "grad_norm": 73113.0703125,
      "learning_rate": 2.6106291413347907e-05,
      "loss": 2.896,
      "step": 9500
    },
    {
      "epoch": 0.6831067695880866,
      "grad_norm": 81571.1015625,
      "learning_rate": 2.590135938247148e-05,
      "loss": 2.9076,
      "step": 10000
    },
    {
      "epoch": 0.7172621080674909,
      "grad_norm": 64426.00390625,
      "learning_rate": 2.5696427351595056e-05,
      "loss": 2.9171,
      "step": 10500
    },
    {
      "epoch": 0.7514174465468952,
      "grad_norm": 93074.1328125,
      "learning_rate": 2.549149532071863e-05,
      "loss": 2.8854,
      "step": 11000
    },
    {
      "epoch": 0.7855727850262996,
      "grad_norm": 74480.90625,
      "learning_rate": 2.5286563289842204e-05,
      "loss": 2.8903,
      "step": 11500
    },
    {
      "epoch": 0.8197281235057039,
      "grad_norm": 60480.57421875,
      "learning_rate": 2.5081631258965777e-05,
      "loss": 2.8803,
      "step": 12000
    },
    {
      "epoch": 0.8538834619851082,
      "grad_norm": 72589.5625,
      "learning_rate": 2.487669922808935e-05,
      "loss": 2.8846,
      "step": 12500
    },
    {
      "epoch": 0.8880388004645126,
      "grad_norm": 261495.0,
      "learning_rate": 2.4671767197212926e-05,
      "loss": 2.8742,
      "step": 13000
    },
    {
      "epoch": 0.9221941389439169,
      "grad_norm": 88349.3515625,
      "learning_rate": 2.4466835166336498e-05,
      "loss": 2.8681,
      "step": 13500
    },
    {
      "epoch": 0.9563494774233212,
      "grad_norm": 63173.14453125,
      "learning_rate": 2.4261903135460074e-05,
      "loss": 2.8662,
      "step": 14000
    },
    {
      "epoch": 0.9905048159027255,
      "grad_norm": 64524.07421875,
      "learning_rate": 2.4056971104583647e-05,
      "loss": 2.8518,
      "step": 14500
    },
    {
      "epoch": 1.0246601543821299,
      "grad_norm": 63634.3828125,
      "learning_rate": 2.3852039073707223e-05,
      "loss": 2.8433,
      "step": 15000
    },
    {
      "epoch": 1.0588154928615343,
      "grad_norm": 64050.49609375,
      "learning_rate": 2.3647107042830792e-05,
      "loss": 2.839,
      "step": 15500
    },
    {
      "epoch": 1.0929708313409385,
      "grad_norm": 96942.6796875,
      "learning_rate": 2.3442175011954368e-05,
      "loss": 2.8417,
      "step": 16000
    },
    {
      "epoch": 1.127126169820343,
      "grad_norm": 78732.9296875,
      "learning_rate": 2.3237242981077944e-05,
      "loss": 2.8398,
      "step": 16500
    },
    {
      "epoch": 1.1612815082997472,
      "grad_norm": 60875.12109375,
      "learning_rate": 2.3032310950201517e-05,
      "loss": 2.834,
      "step": 17000
    },
    {
      "epoch": 1.1954368467791516,
      "grad_norm": 65959.8828125,
      "learning_rate": 2.2827378919325093e-05,
      "loss": 2.8334,
      "step": 17500
    },
    {
      "epoch": 1.2295921852585558,
      "grad_norm": 73971.671875,
      "learning_rate": 2.2622446888448665e-05,
      "loss": 2.8386,
      "step": 18000
    },
    {
      "epoch": 1.2637475237379603,
      "grad_norm": 66101.796875,
      "learning_rate": 2.2417514857572238e-05,
      "loss": 2.8244,
      "step": 18500
    },
    {
      "epoch": 1.2979028622173645,
      "grad_norm": 60993.34765625,
      "learning_rate": 2.2212582826695814e-05,
      "loss": 2.8273,
      "step": 19000
    },
    {
      "epoch": 1.332058200696769,
      "grad_norm": 62250.63671875,
      "learning_rate": 2.2007650795819386e-05,
      "loss": 2.8229,
      "step": 19500
    },
    {
      "epoch": 1.3662135391761732,
      "grad_norm": 66499.296875,
      "learning_rate": 2.1802718764942962e-05,
      "loss": 2.8222,
      "step": 20000
    },
    {
      "epoch": 1.4003688776555776,
      "grad_norm": 67487.609375,
      "learning_rate": 2.1597786734066535e-05,
      "loss": 2.8268,
      "step": 20500
    },
    {
      "epoch": 1.4345242161349818,
      "grad_norm": 182874.140625,
      "learning_rate": 2.1392854703190108e-05,
      "loss": 2.824,
      "step": 21000
    },
    {
      "epoch": 1.4686795546143863,
      "grad_norm": 76711.6953125,
      "learning_rate": 2.1187922672313684e-05,
      "loss": 2.8125,
      "step": 21500
    },
    {
      "epoch": 1.5028348930937905,
      "grad_norm": 66263.171875,
      "learning_rate": 2.0982990641437256e-05,
      "loss": 2.8077,
      "step": 22000
    },
    {
      "epoch": 1.536990231573195,
      "grad_norm": 65354.0234375,
      "learning_rate": 2.0778058610560832e-05,
      "loss": 2.7956,
      "step": 22500
    },
    {
      "epoch": 1.5711455700525994,
      "grad_norm": 59202.94140625,
      "learning_rate": 2.0573126579684405e-05,
      "loss": 2.8106,
      "step": 23000
    },
    {
      "epoch": 1.6053009085320036,
      "grad_norm": 61147.578125,
      "learning_rate": 2.036819454880798e-05,
      "loss": 2.7937,
      "step": 23500
    },
    {
      "epoch": 1.6394562470114078,
      "grad_norm": 74870.5859375,
      "learning_rate": 2.0163262517931553e-05,
      "loss": 2.8108,
      "step": 24000
    },
    {
      "epoch": 1.6736115854908122,
      "grad_norm": 64367.59375,
      "learning_rate": 1.9958330487055126e-05,
      "loss": 2.7969,
      "step": 24500
    },
    {
      "epoch": 1.7077669239702167,
      "grad_norm": 72449.4609375,
      "learning_rate": 1.9753398456178702e-05,
      "loss": 2.8054,
      "step": 25000
    },
    {
      "epoch": 1.741922262449621,
      "grad_norm": 60905.42578125,
      "learning_rate": 1.9548466425302274e-05,
      "loss": 2.8092,
      "step": 25500
    },
    {
      "epoch": 1.7760776009290251,
      "grad_norm": 68186.1796875,
      "learning_rate": 1.934353439442585e-05,
      "loss": 2.7952,
      "step": 26000
    },
    {
      "epoch": 1.8102329394084296,
      "grad_norm": 66666.5859375,
      "learning_rate": 1.9138602363549423e-05,
      "loss": 2.7932,
      "step": 26500
    },
    {
      "epoch": 1.844388277887834,
      "grad_norm": 59217.83984375,
      "learning_rate": 1.8933670332672996e-05,
      "loss": 2.7946,
      "step": 27000
    },
    {
      "epoch": 1.8785436163672382,
      "grad_norm": 72668.5390625,
      "learning_rate": 1.872873830179657e-05,
      "loss": 2.7801,
      "step": 27500
    },
    {
      "epoch": 1.9126989548466424,
      "grad_norm": 77193.296875,
      "learning_rate": 1.8523806270920144e-05,
      "loss": 2.7878,
      "step": 28000
    },
    {
      "epoch": 1.9468542933260469,
      "grad_norm": 72749.6328125,
      "learning_rate": 1.831887424004372e-05,
      "loss": 2.7921,
      "step": 28500
    },
    {
      "epoch": 1.9810096318054513,
      "grad_norm": 72571.953125,
      "learning_rate": 1.8113942209167296e-05,
      "loss": 2.7876,
      "step": 29000
    },
    {
      "epoch": 2.0151649702848555,
      "grad_norm": 61017.828125,
      "learning_rate": 1.7909010178290865e-05,
      "loss": 2.788,
      "step": 29500
    },
    {
      "epoch": 2.0493203087642597,
      "grad_norm": 78625.375,
      "learning_rate": 1.770407814741444e-05,
      "loss": 2.7808,
      "step": 30000
    },
    {
      "epoch": 2.083475647243664,
      "grad_norm": 64491.390625,
      "learning_rate": 1.7499146116538014e-05,
      "loss": 2.7769,
      "step": 30500
    },
    {
      "epoch": 2.1176309857230686,
      "grad_norm": 72385.3984375,
      "learning_rate": 1.729421408566159e-05,
      "loss": 2.7734,
      "step": 31000
    },
    {
      "epoch": 2.151786324202473,
      "grad_norm": 185750.875,
      "learning_rate": 1.7089282054785166e-05,
      "loss": 2.7658,
      "step": 31500
    },
    {
      "epoch": 2.185941662681877,
      "grad_norm": 58431.2421875,
      "learning_rate": 1.6884350023908735e-05,
      "loss": 2.7698,
      "step": 32000
    },
    {
      "epoch": 2.2200970011612817,
      "grad_norm": 54000.55859375,
      "learning_rate": 1.667941799303231e-05,
      "loss": 2.7698,
      "step": 32500
    },
    {
      "epoch": 2.254252339640686,
      "grad_norm": 65944.15625,
      "learning_rate": 1.6474485962155884e-05,
      "loss": 2.7698,
      "step": 33000
    },
    {
      "epoch": 2.28840767812009,
      "grad_norm": 68326.3984375,
      "learning_rate": 1.626955393127946e-05,
      "loss": 2.7679,
      "step": 33500
    },
    {
      "epoch": 2.3225630165994944,
      "grad_norm": 58233.48046875,
      "learning_rate": 1.6064621900403036e-05,
      "loss": 2.7476,
      "step": 34000
    },
    {
      "epoch": 2.3567183550788986,
      "grad_norm": 69323.6484375,
      "learning_rate": 1.585968986952661e-05,
      "loss": 2.764,
      "step": 34500
    },
    {
      "epoch": 2.3908736935583033,
      "grad_norm": 71100.40625,
      "learning_rate": 1.565475783865018e-05,
      "loss": 2.7551,
      "step": 35000
    },
    {
      "epoch": 2.4250290320377075,
      "grad_norm": 80384.5625,
      "learning_rate": 1.5449825807773754e-05,
      "loss": 2.7732,
      "step": 35500
    },
    {
      "epoch": 2.4591843705171117,
      "grad_norm": 79744.140625,
      "learning_rate": 1.524489377689733e-05,
      "loss": 2.7644,
      "step": 36000
    },
    {
      "epoch": 2.4933397089965164,
      "grad_norm": 77906.9453125,
      "learning_rate": 1.5039961746020906e-05,
      "loss": 2.7603,
      "step": 36500
    },
    {
      "epoch": 2.5274950474759206,
      "grad_norm": 58015.12109375,
      "learning_rate": 1.4835029715144476e-05,
      "loss": 2.7589,
      "step": 37000
    },
    {
      "epoch": 2.561650385955325,
      "grad_norm": 80467.46875,
      "learning_rate": 1.4630097684268052e-05,
      "loss": 2.7506,
      "step": 37500
    },
    {
      "epoch": 2.595805724434729,
      "grad_norm": 70567.15625,
      "learning_rate": 1.4425165653391627e-05,
      "loss": 2.7486,
      "step": 38000
    },
    {
      "epoch": 2.6299610629141332,
      "grad_norm": 60766.5703125,
      "learning_rate": 1.42202336225152e-05,
      "loss": 2.7641,
      "step": 38500
    },
    {
      "epoch": 2.664116401393538,
      "grad_norm": 74641.5234375,
      "learning_rate": 1.4015301591638774e-05,
      "loss": 2.7508,
      "step": 39000
    },
    {
      "epoch": 2.698271739872942,
      "grad_norm": 89724.71875,
      "learning_rate": 1.3810369560762346e-05,
      "loss": 2.7432,
      "step": 39500
    },
    {
      "epoch": 2.7324270783523463,
      "grad_norm": 76654.609375,
      "learning_rate": 1.360543752988592e-05,
      "loss": 2.7447,
      "step": 40000
    },
    {
      "epoch": 2.766582416831751,
      "grad_norm": 60094.9453125,
      "learning_rate": 1.3400505499009497e-05,
      "loss": 2.7482,
      "step": 40500
    },
    {
      "epoch": 2.800737755311155,
      "grad_norm": 55943.90234375,
      "learning_rate": 1.3195573468133069e-05,
      "loss": 2.7435,
      "step": 41000
    },
    {
      "epoch": 2.8348930937905594,
      "grad_norm": 66528.7890625,
      "learning_rate": 1.2990641437256643e-05,
      "loss": 2.7523,
      "step": 41500
    },
    {
      "epoch": 2.8690484322699636,
      "grad_norm": 82118.828125,
      "learning_rate": 1.2785709406380218e-05,
      "loss": 2.75,
      "step": 42000
    },
    {
      "epoch": 2.903203770749368,
      "grad_norm": 62641.92578125,
      "learning_rate": 1.258077737550379e-05,
      "loss": 2.7479,
      "step": 42500
    },
    {
      "epoch": 2.9373591092287725,
      "grad_norm": 58500.70703125,
      "learning_rate": 1.2375845344627366e-05,
      "loss": 2.7291,
      "step": 43000
    },
    {
      "epoch": 2.9715144477081767,
      "grad_norm": 82132.9453125,
      "learning_rate": 1.217091331375094e-05,
      "loss": 2.7414,
      "step": 43500
    }
  ],
  "logging_steps": 500,
  "max_steps": 73195,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.754882306034893e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
